# RNN_gradient_problem

# Code README

**Code origin** 

The code for the RNN models use the class nn.rnn from Pytorch. The code for the LSTM models is been adapted from https://towardsdatascience.com/building-a-lstm-by-hand-on-pytorch-59c02a4ec091. Everything else (in particular, the methods for calculating derivatives) comes from our work.

## Classes `top_RNN`, `top_LSTM`, `top_GRU`, `midi_RNN`, `midi_LSTM` and `midi_GRU` (files midi_models and top_models):

The classes include methods for forward pass (`forward`), computing derivatives (`compute_derivatives`), and computing partial derivatives with respect to input features (`compute_dxt_dxk`).

**Usage of Derivative Computations:**
    - compute_derivatives: Computes the derivative of the hidden states with respect to the previous hidden states for each time step.
    - compute_dxt_dxk: Computes $dh_t/dh_k$ by computing the product of derivatives for a given range of time steps, providing partial derivatives of hidden state at time t with respect to hidden state at time k.

Note that for the LSTM models, $dc_t/dc_k$ is computed instead of $dh_t/dh_k$ (see report).

**Differencies between top and midi:** 

In both cases, the architectures of the models are similar. The only differences is the shape of the output. For top models, only the last hidden state is given as input of the linear layer and the outputted when, for midi models, all the hidden states pass through the linear layer and are outputted. 

**Code origin!** 

The code for the RNN models use the class nn.rnn from Pytorch. The computation of


## Class `trainer` (file trainer.py):

This Python code defines a class named `trainer` for training and evaluating a neural network model. The class is designed to handle different types of sequence models, specifically Recurrent Neural Networks (RNN), Long Short-Term Memory networks (LSTM), and Gated Recurrent Units (GRU). It supports two different types of tasks, "top" and "midi," each associated with specific data loaders and loss functions.

### Functionality

1. **Training and Evaluation:**
   - The `train` method trains the model for the specified number of epochs, logging training and validation losses.
   - The `train_1epoch` method performs training for one epoch and call the `compute_grads_eigen` three times during the training.
   - The `eval_1epoch` method evaluates the model for one epoch.

2. **Gradient and Eigenvalue Computation:**
   - The `compute_grads_eigen` method computes gradients and eigenvalues during training.
   - The `plot_magnitude` method visualizes the order of magnitude of gradients at the three different iterations.

3. **Logging and File Handling:**
   - The `get_log` method initializes logging and file handling for recording training progress.



## Data and dataloaders

All the data is saved in the folder raw_data. 
The initialization of the datasets and dataloaders is done during the initialization of the `trainer` object

### 1. top Data Generation and loading (file utils/top_utils.py)

The `generate_sequences` function creates synthetic sequences and corresponding labels. It takes two parameters:
For each sequence, two positions (`pos1` and `pos2`) are randomly selected, and two labels are assigned at those positions. The sequences are generated by randomly choosing characters from the `distractors` list and replacing specific positions with the assigned labels.
The `save_sequences` function stores generated sequences and labels in a CSV file.  

The `load_sequences` function loads sequences and labels from CSV files located in a specified path. It returns six lists: `X_train`, `y_train`, `X_eval`, `X_test`, `y_eval`, and `y_test`.
This function converts a list of characters into a one-hot encoded representation. It maps 'A' to [1, 0, 0, 0, 0, 0], 'B' to [0, 1, 0, 0, 0, 0], and other characters to one-hot encoding based on their ASCII values.
The `load_tensors` function combines loading sequences and labels with their conversion into tensor format. The result of `load_tensors` is used by the function `get_top_dataloaders` to create PyTorch Dataloader objects used for the training. 

### 2. midi sequence processing (file utils/midi_utils.py)

### 2. `convert_midi_to_list` function

The function `convert_midi_to_list` reads notes within MIDI files, truncates the length if it exceeds a specified maximum, and returns a list of one-hot encoded note sequences. The sequences are then saved as npy files using the `save_npy_files` function. 
`load_npy_files` function loads one-hot encoded sequences from npy files. Sequences are then input and output training data for the training. 
The functions `get_max_temperature` and `sample_new_sequence` allow to generate new sequences once a model is trained. The function `convert_list_to_midi`, `play_midi` and `play_music` are used to convert the generated sequence as a MIDI file and to play it.  


### 3. Dataloaders (file dataloaders.py)

This file includes PyTorch datasets and dataloaders for the two tasks

`get_top_dataloaders(batch_size, path, T)`: Loads sequences and labels, creates instances of `top_dataset`, and returns train, eval, and test dataloaders.
`get_midi_dataloaders(batch_size, folder_path)`: Loads MIDI data, creates instances of `midi_dataset`, and returns train, eval, and test dataloaders.


## How to use it?

   ```python

   # Set device (e.g., 'cuda' for GPU)
   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

   # Instantiate a model 
   RNN_top = top_RNN(input_size=6, hidden_dim=50, output_size=2, nonlinearity='tanh')

   # Instantiate the trainer
   my_trainer = trainer(model, model_type, task_type, batch_size=16, nb_epochs=10, T=50, device=device)

   # Lauch the training. The plot_magnitude method is called at the end of the training
   my_trainer.train()
   ```


## Dependencies 

- [torch](https://pytorch.org/) (version 1.13.0): A powerful deep learning framework.
- [numpy](https://numpy.org/) (version 1.23.5): Fundamental package for scientific computing with Python.
- [torchvision](https://pytorch.org/vision/stable/index.html) (version 0.14.0): Datasets, transforms, and models specific to computer vision.
- [tqdm](https://tqdm.github.io/) (version 4.64.1): A fast, extensible progress bar for loops and pipelines.
- [matplotlib](https://matplotlib.org/) (version 3.6.2): A comprehensive library for creating static, animated, and interactive visualizations in Python.
- [pretty_midi](https://github.com/craffel/pretty-midi) (version 0.2.10): A utility for handling MIDI files.
- [pygame](https://www.pygame.org/) (version 2.5.2): A cross-platform set of Python modules designed for writing video games.
- [scikit-learn](https://scikit-learn.org/stable/) (version 1.2.1): Simple and efficient tools for predictive data analysis.
- [IPython](https://ipython.org/) (version 7.28.0): A rich architecture for interactive computing. 

